**Vision Transformer (ViT) from Scratch on CIFAR-10**

This repository contains a modular implementation of a Vision Transformer (ViT) designed specifically for the CIFAR-10 dataset. 

Unlike standard ViT models that use $224 \times 224$ images, this version is optimized for $32 \times 32$ resolution using $4 \times 4$ patches to maintain high sequence density and computational efficiency on laptop GPUs (like the NVIDIA RTX A3000).


**ðŸš€ Key Features**


  Modular Architecture: Clean separation between Patch Embedding, Transformer Encoder, and MLP heads.
  
  Optimized for Small Data: Custom patch sizing ($P=4$) to ensure $64$ tokens per image.
  
  Advanced Training Pipeline: Includes OneCycleLR scheduling, AdamW optimization, and dataset-specific normalization.
  
  Production-Ready: Script for manual "side-loading" of CIFAR-10 to bypass proxy issues.


**Project Structure**
â”œâ”€â”€ data/                  # Local CIFAR-10 binaries

â”œâ”€â”€ src/

â”‚   â”œâ”€â”€ modules/

â”‚   â”‚   â”œâ”€â”€ patching.py    # Patching & Linear Projection

â”‚   â”‚   â”œâ”€â”€ transformer.py # Multi-head Attention & LayerNorm

â”‚   â”‚   â””â”€â”€ mlp.py         # Feed-forward blocks

â”‚   â”œâ”€â”€ dataset.py         # Custom loaders & CIFAR-10 stats

â”‚   â””â”€â”€ model.py           # Model stitching (ViT-Tiny/Small)

â”œâ”€â”€ train.py               # Main training script (with Windows multiprocessing support)

â”œâ”€â”€ infer.py             # Inference script for single images

â”œâ”€â”€ checkpoints/

     â”œâ”€â”€ vit_cifar10_weights.pth # Saved model state_dict
     


**Training Configuration**


  For training on an RTX A3000, the following hyperparameters are recommended:
  
  Patch Size: $4 \times 4$Embedding
  
  Dim: $256$
  
  Batch Size: $128$
  
  Optimizer: AdamW ($wd=0.05$)
  
  LR Scheduler: OneCycleLR (Max $LR=5e-4$)
  
  Normalization: Mean (0.4914, 0.4822, 0.4465), Std (0.2023, 0.1994, 0.2010)
